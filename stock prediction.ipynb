{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock data written to CSV file successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# Your Alpha Vantage API key\n",
    "api_key = '5SRJ25KFE2K282K0'\n",
    "\n",
    "# Function to fetch data for a given stock symbol\n",
    "def fetch_stock_data(symbol):\n",
    "    url = 'https://www.alphavantage.co/query'\n",
    "    params = {\n",
    "        'function': 'TIME_SERIES_DAILY',\n",
    "        'symbol': symbol,\n",
    "        'apikey': api_key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"Time Series (Daily)\" in data:\n",
    "        return data['Time Series (Daily)']\n",
    "    else:\n",
    "        print(f\"Error fetching data for {symbol}: {data.get('Note', 'No data found')}\")\n",
    "        return None\n",
    "\n",
    "# List of stock symbols\n",
    "stock_symbols = ['AAPL', 'TSLA', 'JPM', 'PFE', 'XOM']\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "with open('stock_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writerow(['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    # Fetch data for each symbol and write to CSV\n",
    "    for symbol in stock_symbols:\n",
    "        stock_data = fetch_stock_data(symbol)\n",
    "        if stock_data:\n",
    "            for date, daily_data in stock_data.items():\n",
    "                writer.writerow([\n",
    "                    symbol,\n",
    "                    date,\n",
    "                    daily_data['1. open'],\n",
    "                    daily_data['2. high'],\n",
    "                    daily_data['3. low'],\n",
    "                    daily_data['4. close'],\n",
    "                    daily_data['5. volume']\n",
    "                ])\n",
    "\n",
    "print(\"Stock data written to CSV file successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data written to CSV file successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# Your News API key\n",
    "api_key = 'a9dd58fced9045d18b1516c1fb48b6cc'\n",
    "\n",
    "# The endpoint for everything (general search across news sources)\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "# Parameters for the request\n",
    "params = {\n",
    "    'q': 'AAPL OR TSLA OR JPM OR PFE OR XOM',  # Keywords or phrases to search for\n",
    "    'sortBy': 'relevancy',  # Sort results by relevancy, popularity, or publishedAt\n",
    "    'language': 'en',  # Language of the articles\n",
    "    'apiKey': api_key\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Check for successful response\n",
    "if response.status_code == 200:\n",
    "    # Extract articles\n",
    "    articles = data['articles']\n",
    "    \n",
    "    # Define the CSV file headers\n",
    "    csv_headers = ['Title', 'Source', 'URL', 'Published At', 'Description']\n",
    "    \n",
    "    # Open a new CSV file for writing\n",
    "    with open('news_articles.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow(csv_headers)\n",
    "        \n",
    "        # Write the article data\n",
    "        for article in articles:\n",
    "            writer.writerow([\n",
    "                article['title'],\n",
    "                article['source']['name'],\n",
    "                article['url'],\n",
    "                article['publishedAt'],\n",
    "                article['description']\n",
    "            ])\n",
    "    \n",
    "    print(\"New data written to CSV file successfully.\")\n",
    "else:\n",
    "    print(f\"Error: {data['message']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol        Date    Open      High     Low   Close    Volume\n",
      "0   AAPL  2024-08-01  224.30  224.4505  217.02  218.36  55541885\n",
      "1   AAPL  2024-07-31  221.44  223.8200  220.63  222.08  50036262\n",
      "2   AAPL  2024-07-30  219.19  220.3250  216.12  218.80  41643840\n",
      "3   AAPL  2024-07-29  216.96  219.3000  215.75  218.24  36311778\n",
      "4   AAPL  2024-07-26  218.70  219.4900  216.01  217.96  41601345\n",
      "        Date Symbol      Open      High       Low     Close    Volume  \\\n",
      "0 2024-07-05   AAPL  1.187415  1.209815  1.228027  1.253106  0.377330   \n",
      "1 2024-07-03   AAPL  1.162919  1.138307  1.188594  1.182101 -0.190375   \n",
      "2 2024-07-02   AAPL  1.105761  1.121233  1.129445  1.163127  0.319033   \n",
      "3 2024-07-01   AAPL  1.045485  1.079349  1.081584  1.110947  0.377097   \n",
      "4 2024-06-28   AAPL  1.100119  1.058335  1.057202  1.020078  0.922559   \n",
      "\n",
      "   Close_Lag1  Close_Lag2  Daily_Return  ...       ROC    SMA_20      EMA_20  \\\n",
      "0      227.82      228.68     -0.649636  ...  0.591085  225.0480  225.968841   \n",
      "1      226.34      227.82     -2.116285  ... -1.076085  225.2075  225.547999   \n",
      "2      221.55      226.34     -0.577748  ... -1.801079  225.1170  225.045332   \n",
      "3      220.27      221.55     -1.598039  ... -3.314301  225.0145  224.255301   \n",
      "4      216.75      220.27     -2.828143  ... -7.977980  224.6335  222.956701   \n",
      "\n",
      "         RSI      EMA_12      EMA_26      MACD  MACD_Signal  Volatility  \\\n",
      "0  61.349064  227.637636  224.953812  2.683825     2.706268    5.669610   \n",
      "1  53.522116  226.701077  224.701678  1.999399     2.564894    5.514302   \n",
      "2  43.686734  225.711681  224.373405  1.338275     2.319570    5.582759   \n",
      "3  40.989753  224.332960  223.808709  0.524252     1.960507    5.721929   \n",
      "4  35.051321  222.223274  222.831767 -0.608493     1.446707    6.409177   \n",
      "\n",
      "   Volume_Mean  \n",
      "0  52645817.70  \n",
      "1  51737213.50  \n",
      "2  52137709.30  \n",
      "3  53075663.75  \n",
      "4  55387210.75  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_stock = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# Display the first few rows to ensure data is loaded correctly\n",
    "print(df_stock.head())\n",
    "\n",
    "# Fill missing values with the forward fill method\n",
    "df_stock.ffill(inplace=True)\n",
    "\n",
    "# Convert 'Date' to datetime and set it as index\n",
    "df_stock['Date'] = pd.to_datetime(df_stock['Date'])\n",
    "df_stock.set_index('Date', inplace=True)\n",
    "\n",
    "# Define the window size for the moving average (20 days)\n",
    "window_size = 20\n",
    "\n",
    "# Adding previous days' prices as features\n",
    "df_stock['Close_Lag1'] = df_stock['Close'].shift(1)\n",
    "df_stock['Close_Lag2'] = df_stock['Close'].shift(2)\n",
    "\n",
    "# Calculate daily return\n",
    "df_stock['Daily_Return'] = df_stock['Close'].pct_change() * 100  # Multiply by 100 for percentage\n",
    "\n",
    "# Adding lagged returns as features\n",
    "df_stock['Return_Lag1'] = df_stock['Daily_Return'].shift(1)\n",
    "df_stock['Return_Lag2'] = df_stock['Daily_Return'].shift(2)\n",
    "\n",
    "# Calculate the moving average\n",
    "df_stock['Moving_Avg'] = df_stock['Close'].rolling(window=window_size).mean()\n",
    "\n",
    "# Calculate Rate of Change\n",
    "df_stock['ROC'] = df_stock['Close'].pct_change(periods=12) * 100\n",
    "\n",
    "# Extracting meaningful features for modeling stock prices\n",
    "df_stock['SMA_20'] = df_stock['Close'].rolling(window=window_size).mean()\n",
    "df_stock['EMA_20'] = df_stock['Close'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "def calculate_rsi(df, period=14):\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "# Measure Relative Strength Index (RSI)\n",
    "df_stock['RSI'] = calculate_rsi(df_stock)\n",
    "\n",
    "# Calculate 12-day and 26-day EMA\n",
    "df_stock['EMA_12'] = df_stock['Close'].ewm(span=12, adjust=False).mean()\n",
    "df_stock['EMA_26'] = df_stock['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "# Measure Moving Average Convergence Divergence (MACD)\n",
    "df_stock['MACD'] = df_stock['EMA_12'] - df_stock['EMA_26']\n",
    "df_stock['MACD_Signal'] = df_stock['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Measure Volatility and Volume_Mean\n",
    "df_stock['Volatility'] = df_stock['Close'].rolling(window=window_size).std()\n",
    "df_stock['Volume_Mean'] = df_stock['Volume'].rolling(window=window_size).mean()\n",
    "\n",
    "# Drop rows with NaN values introduced by shifting\n",
    "df_stock.dropna(inplace=True)\n",
    "\n",
    "# Reset index to make 'Date' a column again before normalization\n",
    "df_stock.reset_index(inplace=True)\n",
    "\n",
    "# Columns to normalize\n",
    "columns_to_normalize = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "# Ensure the columns to normalize exist\n",
    "columns_to_normalize = [col for col in columns_to_normalize if col in df_stock.columns]\n",
    "\n",
    "if not columns_to_normalize:\n",
    "    print(\"No columns to normalize.\")\n",
    "else:\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    df_stock[columns_to_normalize] = scaler.fit_transform(df_stock[columns_to_normalize])\n",
    "\n",
    "    # Print the normalized DataFrame\n",
    "    print(df_stock.head())\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df_stock.to_csv('cleaned_stock_article.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akinahomwabella/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/akinahomwabella/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/akinahomwabella/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Cleaned_Title  \\\n",
      "0  exxonmobil xom reveals hammerhead project deve...   \n",
      "1       tesla robotaxi wont ready scale 2030 analyst   \n",
      "2          tesla building robotaxi there hidden cost   \n",
      "3  cathie wood say wouldnt sold nvidia stake know...   \n",
      "4       td cowen raise aapl target 250 ai china sale   \n",
      "\n",
      "                                 Cleaned_Description  \n",
      "0  exxonmobils xom development plan hammerhead in...  \n",
      "1  tesla tsla share trading higher friday despite...  \n",
      "2  tesla tsla report latest quarterly report tues...  \n",
      "3  cathie wood investment fund ark invest sold le...  \n",
      "4  investment analyst firm td cowen predicts appl...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "# Download stopwords and punkt if not already available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV file into a DataFrame (replace 'news_article.csv' with your actual file)\n",
    "df_text = pd.read_csv('news_articles.csv')\n",
    "# Assuming the columns with text data are named 'Title' and 'Description', replace with actual column names\n",
    "text_columns = ['Title', 'Description']\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Check if the text is a string (to avoid errors with NaN or other types)\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Tokenize the text\n",
    "        words = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize the words\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        # Join the words back into a single string\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply the preprocessing function to each specified text column\n",
    "for column in text_columns:\n",
    "    df_text[f'Cleaned_{column}'] = df_text[column].apply(preprocess_text)\n",
    "df_text.drop(columns=[\"URL\", \"Source\"], inplace=True)\n",
    "# Display the cleaned text columns\n",
    "print(df_text[[f'Cleaned_{col}' for col in text_columns]].head())\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df_text.to_csv('cleaned_news_article.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/var/folders/pp/ckr57fqj6vb03g9vr4mbnvjw0000gn/T/ipykernel_95609/2201438188.py:43: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df_cleaned_text['Week'] = df_cleaned_text['Date'].dt.to_period('W').apply(lambda r: r.start_time)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Title_Sentiment_Score  Description_Sentiment_Score  \\\n",
      "0                 0.0000                      -0.3818   \n",
      "1                -0.2755                      -0.0258   \n",
      "2                 0.0000                       0.0000   \n",
      "3                 0.5719                       0.2263   \n",
      "4                 0.0000                       0.7650   \n",
      "\n",
      "                            Word2Vec_Embedding_Title  \\\n",
      "0  [-0.023931492, -0.019036919, 0.07065436, -0.02...   \n",
      "1  [-0.011779225, -0.031012023, -0.09955558, -0.0...   \n",
      "2  [-0.062481813, 0.034244537, -0.031933334, 0.00...   \n",
      "3  [0.026826339, -0.021405887, -0.026352998, -0.0...   \n",
      "4  [-0.07889189, -0.034725867, 0.009962358, -0.06...   \n",
      "\n",
      "                      Word2Vec_Embedding_Description  \n",
      "0  [-0.018905027, 0.016774863, 0.024963025, -0.01...  \n",
      "1  [-0.050417252, -0.05723135, -0.00022364523, 0....  \n",
      "2  [-0.022078425, -0.018386573, -0.018221067, 0.0...  \n",
      "3  [0.096326, -0.015384293, -0.072952285, 0.04721...  \n",
      "4  [-0.044582628, -0.017881077, 0.0332282, -0.027...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Load the CSV files with cleaned data\n",
    "df_cleaned_text = pd.read_csv('cleaned_news_article.csv')\n",
    "df_cleaned_stock = pd.read_csv('cleaned_stock_article.csv')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate sentiment scores\n",
    "def sentiment_analysis(text):\n",
    "    if isinstance(text, str):\n",
    "        return analyzer.polarity_scores(text)['compound']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply sentiment analysis to text data\n",
    "df_cleaned_text['Title_Sentiment_Score'] = df_cleaned_text['Cleaned_Title'].apply(sentiment_analysis)\n",
    "df_cleaned_text['Description_Sentiment_Score'] = df_cleaned_text['Cleaned_Description'].apply(sentiment_analysis)\n",
    "\n",
    "# Convert 'Published At' column to datetime format\n",
    "df_cleaned_text['Date'] = pd.to_datetime(df_cleaned_text['Published At'], errors='coerce')\n",
    "\n",
    "\n",
    "# Calculate daily average sentiment scores\n",
    "daily_sentiment = df_cleaned_text.groupby(df_cleaned_text['Date'].dt.date)['Title_Sentiment_Score'].mean()\n",
    "daily_sentiment_df = daily_sentiment.reset_index()\n",
    "daily_sentiment_df.columns = ['Date', 'Daily_Sentiment_Score']\n",
    "\n",
    "# Convert 'Date' in daily_sentiment_df to datetime format for merging\n",
    "daily_sentiment_df['Date'] = pd.to_datetime(daily_sentiment_df['Date'])\n",
    "\n",
    "# Merge daily sentiment scores with stock data\n",
    "df_cleaned_stock['Date'] = pd.to_datetime(df_cleaned_stock['Date'], errors='coerce')\n",
    "df_cleaned_stock = pd.merge(df_cleaned_stock, daily_sentiment_df, on='Date', how='left')\n",
    "\n",
    "# Aggregate sentiment scores by week\n",
    "df_cleaned_text['Week'] = df_cleaned_text['Date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly_sentiment = df_cleaned_text.groupby('Week')['Title_Sentiment_Score'].mean()\n",
    "weekly_sentiment_df = weekly_sentiment.reset_index()\n",
    "weekly_sentiment_df.columns = ['Week', 'Weekly_Sentiment_Score']\n",
    "\n",
    "# Convert 'Week' in weekly_sentiment_df to datetime format for merging\n",
    "weekly_sentiment_df['Week'] = pd.to_datetime(weekly_sentiment_df['Week'])\n",
    "\n",
    "# Merge weekly sentiment scores with stock data\n",
    "df_cleaned_stock['Week'] = df_cleaned_stock['Date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "df_cleaned_stock = pd.merge(df_cleaned_stock, weekly_sentiment_df, on='Week', how='left')\n",
    "\n",
    "# Load the Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define a function to get sentence embeddings using Sentence-BERT\n",
    "def get_sentence_embedding(sentence):\n",
    "    if isinstance(sentence, str):\n",
    "        return model.encode(sentence)\n",
    "    else:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "\n",
    "# Compute Word2Vec embeddings for each text\n",
    "df_cleaned_text['Word2Vec_Embedding_Title'] = df_cleaned_text['Cleaned_Title'].apply(lambda x: get_sentence_embedding(x))\n",
    "df_cleaned_text['Word2Vec_Embedding_Description'] = df_cleaned_text['Cleaned_Description'].apply(lambda x: get_sentence_embedding(x))\n",
    "\n",
    "# Drop the original text columns\n",
    "df_cleaned_text.drop(['Cleaned_Title', 'Cleaned_Description'], axis=1, inplace=True)\n",
    "# Print the resulting DataFrame with sentiment scores and embeddings\n",
    "print(df_cleaned_text[['Title_Sentiment_Score', 'Description_Sentiment_Score', 'Word2Vec_Embedding_Title','Word2Vec_Embedding_Description']].head())\n",
    "\n",
    "# Save the DataFrame with sentiment scores and embeddings to a new CSV file\n",
    "df_cleaned_text.to_csv('news_article_with_sentiment.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame head:\n",
      "        Date Symbol      Open      High       Low     Close    Volume  \\\n",
      "0 2024-07-05   AAPL  1.187415  1.209815  1.228027  1.253106  0.377330   \n",
      "1 2024-07-03   AAPL  1.162919  1.138307  1.188594  1.182101 -0.190375   \n",
      "2 2024-07-02   AAPL  1.105761  1.121233  1.129445  1.163127  0.319033   \n",
      "3 2024-07-01   AAPL  1.045485  1.079349  1.081584  1.110947  0.377097   \n",
      "4 2024-06-28   AAPL  1.100119  1.058335  1.057202  1.020078  0.922559   \n",
      "\n",
      "   Close_Lag1  Close_Lag2  Daily_Return  ...  Volume_Mean  \\\n",
      "0      227.82      228.68     -0.649636  ...  52645817.70   \n",
      "1      226.34      227.82     -2.116285  ...  51737213.50   \n",
      "2      221.55      226.34     -0.577748  ...  52137709.30   \n",
      "3      220.27      221.55     -1.598039  ...  53075663.75   \n",
      "4      216.75      220.27     -2.828143  ...  55387210.75   \n",
      "\n",
      "   Title_Sentiment_Score  Description_Sentiment_Score                Date  \\\n",
      "0                 0.0000                      -0.3818 2024-07-16 13:11:00   \n",
      "1                -0.2755                      -0.0258 2024-07-12 15:19:12   \n",
      "2                 0.0000                       0.0000 2024-07-20 12:00:40   \n",
      "3                 0.5719                       0.2263 2024-07-15 07:32:21   \n",
      "4                 0.0000                       0.7650 2024-07-29 11:01:32   \n",
      "\n",
      "         Week                           Word2Vec_Embedding_Title  \\\n",
      "0  2024-07-15  [-2.39314921e-02 -1.90369189e-02  7.06543624e-...   \n",
      "1  2024-07-08  [-1.17792254e-02 -3.10120229e-02 -9.95555818e-...   \n",
      "2  2024-07-15  [-6.24818131e-02  3.42445374e-02 -3.19333337e-...   \n",
      "3  2024-07-15  [ 2.68263388e-02 -2.14058869e-02 -2.63529979e-...   \n",
      "4  2024-07-29  [-7.88918883e-02 -3.47258672e-02  9.96235758e-...   \n",
      "\n",
      "                      Word2Vec_Embedding_Description    Year  Month   Day  \n",
      "0  [-1.89050268e-02  1.67748630e-02  2.49630250e-...  2024.0    7.0  16.0  \n",
      "1  [-5.04172519e-02 -5.72313517e-02 -2.23645227e-...  2024.0    7.0  12.0  \n",
      "2  [-2.20784247e-02 -1.83865726e-02 -1.82210673e-...  2024.0    7.0  20.0  \n",
      "3  [ 9.63260010e-02 -1.53842932e-02 -7.29522854e-...  2024.0    7.0  15.0  \n",
      "4  [-4.45826277e-02 -1.78810768e-02  3.32281999e-...  2024.0    7.0  29.0  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "Merged DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 481 entries, 0 to 480\n",
      "Data columns (total 32 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   Date                            481 non-null    datetime64[ns]\n",
      " 1   Symbol                          481 non-null    object        \n",
      " 2   Open                            481 non-null    float64       \n",
      " 3   High                            481 non-null    float64       \n",
      " 4   Low                             481 non-null    float64       \n",
      " 5   Close                           481 non-null    float64       \n",
      " 6   Volume                          481 non-null    float64       \n",
      " 7   Close_Lag1                      481 non-null    float64       \n",
      " 8   Close_Lag2                      481 non-null    float64       \n",
      " 9   Daily_Return                    481 non-null    float64       \n",
      " 10  Return_Lag1                     481 non-null    float64       \n",
      " 11  Return_Lag2                     481 non-null    float64       \n",
      " 12  Moving_Avg                      481 non-null    float64       \n",
      " 13  ROC                             481 non-null    float64       \n",
      " 14  SMA_20                          481 non-null    float64       \n",
      " 15  EMA_20                          481 non-null    float64       \n",
      " 16  RSI                             481 non-null    float64       \n",
      " 17  EMA_12                          481 non-null    float64       \n",
      " 18  EMA_26                          481 non-null    float64       \n",
      " 19  MACD                            481 non-null    float64       \n",
      " 20  MACD_Signal                     481 non-null    float64       \n",
      " 21  Volatility                      481 non-null    float64       \n",
      " 22  Volume_Mean                     481 non-null    float64       \n",
      " 23  Title_Sentiment_Score           100 non-null    float64       \n",
      " 24  Description_Sentiment_Score     95 non-null     float64       \n",
      " 25  Date                            100 non-null    datetime64[ns]\n",
      " 26  Week                            100 non-null    object        \n",
      " 27  Word2Vec_Embedding_Title        100 non-null    object        \n",
      " 28  Word2Vec_Embedding_Description  100 non-null    object        \n",
      " 29  Year                            100 non-null    float64       \n",
      " 30  Month                           100 non-null    float64       \n",
      " 31  Day                             100 non-null    float64       \n",
      "dtypes: datetime64[ns](2), float64(26), object(4)\n",
      "memory usage: 120.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df1 = pd.read_csv('cleaned_stock_article.csv')\n",
    "df2 = pd.read_csv('news_article_with_sentiment.csv')\n",
    "\n",
    "df2.drop(columns=[\"Title\", \"Description\",\"Published At\"], inplace=True)\n",
    "# Convert 'Date' columns to datetime format for consistency\n",
    "df1['Date'] = pd.to_datetime(df1['Date'], errors='coerce')\n",
    "df2['Date'] = pd.to_datetime(df2['Date'], errors='coerce')\n",
    "\n",
    "# Ensure both 'Date' columns are naive datetime (without timezone)\n",
    "df2['Date'] = df2['Date'].dt.tz_localize(None)\n",
    "df2['Year'] = df2['Date'].dt.year\n",
    "df2['Month'] = df2['Date'].dt.month\n",
    "df2['Day'] = df2['Date'].dt.day\n",
    "concat_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# Step 2: Verify the merge\n",
    "print(\"Merged DataFrame head:\")\n",
    "print(concat_df.head())\n",
    "print(\"Merged DataFrame info:\")\n",
    "print(concat_df.info())\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "concat_df.to_csv('merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Rankings (Linear Regression): [ 1  1  1  3  8 16 15  9 10 14  1 11  1  2 13  1  1  1  4 12 18  5  7 19\n",
      "  6 17  1 21 20  1 22]\n",
      "Selected Features (Linear Regression): Index(['Open', 'High', 'Low', 'Moving_Avg', 'SMA_20', 'EMA_12', 'EMA_26',\n",
      "       'MACD', 'Symbol_AAPL', 'Symbol_TSLA'],\n",
      "      dtype='object')\n",
      "Feature Rankings (Decision Tree): [ 1  6  1  1  1  1 12  3  7  1  8 10  1  4  5  1 13  2  1  1 14 17  9 16\n",
      " 15 11 18 19 20 21 22]\n",
      "Selected Features (Decision Tree): Index(['Open', 'Low', 'Close', 'Volume', 'Close_Lag1', 'Return_Lag2', 'SMA_20',\n",
      "       'EMA_12', 'MACD_Signal', 'Volatility'],\n",
      "      dtype='object')\n",
      "Linear Regression Model Accuracy: 0.9841223040294053\n",
      "Decision Tree Model Accuracy: 0.5977344432569907\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('merged.csv')\n",
    "\n",
    "# Convert 'Date' to datetime and extract year, month, and day\n",
    "df['Date'] = pd.to_datetime(df['Date'])  # Convert to datetime\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df.drop(columns=['Date'], inplace=True)  # Drop the original Date column\n",
    "# One-hot encode the 'Symbol' column\n",
    "df = pd.get_dummies(df, columns=['Symbol'])\n",
    "# Shift the 'Close' prices to create 'Future_Close'\n",
    "df['Future_Close'] = df['Close'].shift(-1)\n",
    "# Drop the last row which will have NaN value in 'Future_Close'\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define target and features\n",
    "y = df['Future_Close']\n",
    "X = df.drop(['Future_Close'], axis=1)\n",
    "# Ensure all feature columns are numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X.dropna(inplace=True, axis=1)  # Drop columns with NaN values after conversion\n",
    "\n",
    "# Initialize the models\n",
    "linear_model = LinearRegression()\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Perform RFE with Linear Regression\n",
    "rfe_linear = RFE(estimator=linear_model, n_features_to_select=10)\n",
    "X_rfe_linear = rfe_linear.fit_transform(X, y)\n",
    "ranking_linear = rfe_linear.ranking_\n",
    "\n",
    "# Print feature rankings and selected features for Linear Regression\n",
    "print(\"Feature Rankings (Linear Regression):\", ranking_linear)\n",
    "print(\"Selected Features (Linear Regression):\", X.columns[rfe_linear.support_])\n",
    "\n",
    "# Perform RFE with Decision Tree\n",
    "rfe_tree = RFE(estimator=decision_tree_model, n_features_to_select=10)\n",
    "X_rfe_tree = rfe_tree.fit_transform(X, y)\n",
    "ranking_tree = rfe_tree.ranking_\n",
    "\n",
    "# Print feature rankings and selected features for Decision Tree\n",
    "print(\"Feature Rankings (Decision Tree):\", ranking_tree)\n",
    "print(\"Selected Features (Decision Tree):\", X.columns[rfe_tree.support_])\n",
    "\n",
    "# Split the data into training and test sets for both models\n",
    "X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(X_rfe_linear, y, test_size=0.2, random_state=42)\n",
    "X_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_rfe_tree, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the models on the selected features\n",
    "linear_model.fit(X_train_linear, y_train_linear)\n",
    "decision_tree_model.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "# Evaluate the models\n",
    "linear_score = linear_model.score(X_test_linear, y_test_linear)\n",
    "decision_tree_score = decision_tree_model.score(X_test_tree, y_test_tree)\n",
    "\n",
    "print(\"Linear Regression Model Accuracy:\", linear_score)\n",
    "print(\"Decision Tree Model Accuracy:\", decision_tree_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open                                         float64\n",
      "High                                         float64\n",
      "Low                                          float64\n",
      "Close                                        float64\n",
      "Volume                                       float64\n",
      "                                              ...   \n",
      "Word2Vec_Embedding_Description_feature_27    float64\n",
      "Word2Vec_Embedding_Description_feature_28    float64\n",
      "Word2Vec_Embedding_Description_feature_29    float64\n",
      "Word2Vec_Embedding_Description_feature_30    float64\n",
      "Word2Vec_Embedding_Description_feature_31    float64\n",
      "Length: 65, dtype: object\n",
      "Mean Squared Error: 0.026234448537510606\n",
      "                                     Feature  Importance\n",
      "17                                      MACD    0.381805\n",
      "2                                        Low    0.305199\n",
      "0                                       Open    0.161827\n",
      "36  Word2Vec_Embedding_Description_feature_3    0.084479\n",
      "5                                 Close_Lag1    0.032280\n",
      "..                                       ...         ...\n",
      "34  Word2Vec_Embedding_Description_feature_1    0.000000\n",
      "26                                     Month    0.000000\n",
      "28                               Symbol_AAPL    0.000000\n",
      "31                               Symbol_TSLA    0.000000\n",
      "32                                Symbol_XOM    0.000000\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Best score: 0.013219213347773007\n",
      "Mean Squared Error with loaded model: 0.026234448537510606\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('merged.csv')\n",
    "\n",
    "# Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df.drop(columns=['Date'], inplace=True)\n",
    "df = pd.get_dummies(df, columns=['Symbol'])\n",
    "df['Future_Close'] = df['Close'].shift(-1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define target and features\n",
    "y = df['Future_Close']\n",
    "X = df.drop(['Future_Close'], axis=1)\n",
    "\n",
    "# Label encoding for categorical columns\n",
    "label_encoders = {}\n",
    "for column in ['Date.1', 'Week']:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Define embedding dimension\n",
    "embedding_dim = 32  \n",
    "\n",
    "# Flatten embeddings\n",
    "def flatten_embeddings(df, column_name, num_features):\n",
    "    # Check the type of embeddings and convert accordingly\n",
    "    def parse_embedding(embedding):\n",
    "        if isinstance(embedding, str):\n",
    "            try:\n",
    "                return np.fromstring(embedding.strip('[]'), sep=' ')\n",
    "            except:\n",
    "                return np.array([])\n",
    "        else:\n",
    "            return np.array(embedding)\n",
    "    \n",
    "    embeddings = df[column_name].apply(parse_embedding)\n",
    "    for i in range(num_features):\n",
    "        df[f'{column_name}_feature_{i}'] = embeddings.apply(lambda x: x[i] if len(x) > i else np.nan)\n",
    "\n",
    "# Apply the flatten_embeddings function\n",
    "flatten_embeddings(df, 'Word2Vec_Embedding_Description', num_features=embedding_dim)\n",
    "\n",
    "# Remove original embedding columns\n",
    "df = df.drop(columns=['Word2Vec_Embedding_Title', 'Word2Vec_Embedding_Description'])\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "X = df.drop(['Future_Close'], axis=1)\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check data types\n",
    "print(X.dtypes)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse')\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Feature importance\n",
    "importances = xgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(importances_df.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=3,\n",
    "                           verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {-grid_search.best_score_}\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(xgb_model, 'xgb_model.pkl')\n",
    "\n",
    "# Load the model (example of how to load it later)\n",
    "loaded_model = joblib.load('xgb_model.pkl')\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "mse_loaded = mean_squared_error(y_test, y_pred_loaded)\n",
    "print(f\"Mean Squared Error with loaded model: {mse_loaded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (3.4.1)\n",
      "Requirement already satisfied: absl-py in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: packaging in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: rich in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (8.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.18,>=2.17->tensorflow) (3.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akinahomwabella/Library/Python/3.9/lib/python/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - loss: 0.6662 - val_loss: 0.4831\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2857 - val_loss: 0.2247\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1693 - val_loss: 0.1823\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2270 - val_loss: 0.2042\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2639 - val_loss: 0.1962\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2457 - val_loss: 0.1814\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1916 - val_loss: 0.1855\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1714 - val_loss: 0.2127\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1672 - val_loss: 0.2477\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1666 - val_loss: 0.2744\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "LSTM Mean Squared Error: 0.2978300154209137\n"
     ]
    }
   ],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "df = pd.read_csv('merged.csv')\n",
    "\n",
    "# Convert Date to datetime and create new features\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df.drop(columns=['Date'], inplace=True)\n",
    "df['Future_Close'] = df['Close'].shift(-1)\n",
    "df = pd.get_dummies(df, columns=['Symbol'])\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Label encoding for categorical columns\n",
    "label_encoders = {}\n",
    "for column in ['Date.1', 'Week']:\n",
    "    if df[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        df[column] = le.fit_transform(df[column])\n",
    "        label_encoders[column] = le\n",
    "\n",
    "# Define embedding dimension\n",
    "embedding_dim = 32  \n",
    "\n",
    "# Flatten embeddings\n",
    "def flatten_embeddings(df, column_name, num_features):\n",
    "    def parse_embedding(embedding):\n",
    "        if isinstance(embedding, str):\n",
    "            try:\n",
    "                return np.fromstring(embedding.strip('[]'), sep=' ')\n",
    "            except:\n",
    "                return np.array([])\n",
    "        else:\n",
    "            return np.array(embedding)\n",
    "    \n",
    "    embeddings = df[column_name].apply(parse_embedding)\n",
    "    for i in range(num_features):\n",
    "        df[f'{column_name}_feature_{i}'] = embeddings.apply(lambda x: x[i] if len(x) > i else np.nan)\n",
    "\n",
    "# Apply the flatten_embeddings function\n",
    "flatten_embeddings(df, 'Word2Vec_Embedding_Description', num_features=embedding_dim)\n",
    "\n",
    "# Remove original embedding columns\n",
    "df = df.drop(columns=['Word2Vec_Embedding_Title', 'Word2Vec_Embedding_Description'])\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "X = df.drop(['Future_Close'], axis=1)\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Create lag features for LSTM input\n",
    "def create_lagged_features(df, lag=1):\n",
    "    for i in range(1, lag+1):\n",
    "        df[f'Close_Lag{i}'] = df['Close'].shift(i)\n",
    "    return df\n",
    "\n",
    "# Add lag features\n",
    "df = create_lagged_features(df, lag=5)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define target and features\n",
    "y = df['Future_Close']\n",
    "X = df.drop(['Future_Close'], axis=1)\n",
    "\n",
    "# Convert to NumPy arrays and ensure consistent dtype\n",
    "X = X.values.astype(np.float32)\n",
    "y = y.values.astype(np.float32)\n",
    "\n",
    "# Reshape data for LSTM\n",
    "timesteps = 5  # Number of timesteps for LSTM\n",
    "num_features = X.shape[1]\n",
    "\n",
    "def create_lstm_dataset(X, y, timesteps):\n",
    "    X_lstm, y_lstm = [], []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        X_lstm.append(X[i:i+timesteps])\n",
    "        y_lstm.append(y[i+timesteps])\n",
    "    return np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "X_lstm, y_lstm = create_lstm_dataset(X, y, timesteps)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM model\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Initialize and train the model\n",
    "input_shape = (timesteps, num_features)\n",
    "lstm_model = build_lstm_model(input_shape)\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lstm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"LSTM Mean Squared Error: {mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
